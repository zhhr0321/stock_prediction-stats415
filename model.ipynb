{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's the CNN-LSTM model with comments.\n",
    "\n",
    "(we put it seperate to make it more clear compare to the experiment.ipynb, the output of the code can be seen in the original file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Define the number of days to use for training in each sample\n",
    "DAYS_FOR_TRAIN = 40 \n",
    "\n",
    "# Compute Relative Strength Index (RSI) for a given series\n",
    "def compute_RSI(series, window=14):\n",
    "    delta = series.diff().dropna()\n",
    "    up = delta.where(delta > 0, 0.0)\n",
    "    down = -delta.where(delta < 0, 0.0)\n",
    "    roll_up = up.rolling(window).mean()\n",
    "    roll_down = down.rolling(window).mean()\n",
    "    RS = roll_up / roll_down\n",
    "    RSI = 100.0 - (100.0 / (1.0 + RS))\n",
    "    return RSI\n",
    "\n",
    "# Create dataset for training and testing\n",
    "def create_dataset(data, close_prices, days_for_train):\n",
    "    dataset_x, dataset_y = [], []\n",
    "    for i in range(len(data) - days_for_train):\n",
    "        _x = data[i:(i + days_for_train), :]\n",
    "        dataset_x.append(_x)\n",
    "        dataset_y.append(close_prices[i + days_for_train])\n",
    "    return np.array(dataset_x), np.array(dataset_y)\n",
    "\n",
    "# Define the Enhanced CNN-LSTM model\n",
    "class Enhanced_CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=1, cnn_filters=64):\n",
    "        super().__init__()\n",
    "        # Different branches of CNN with different kernel sizes\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=cnn_filters, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=cnn_filters, out_channels=cnn_filters, kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=cnn_filters, kernel_size=5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(cnn_filters * 2)  # Batch Normalization\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(cnn_filters * 2, hidden_size, num_layers=2, batch_first=False, dropout=0.2, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x for CNN\n",
    "        x = x.permute(1,2,0)  # (batch, feature_dim, seq_len)\n",
    "        out1 = self.branch1(x)  # (batch, cnn_filters, seq_len-4)\n",
    "        out2 = self.branch2(x)  # (batch, cnn_filters, seq_len-4)\n",
    "        \n",
    "        # Ensure the sequences have the same length\n",
    "        min_len = min(out1.shape[2], out2.shape[2])\n",
    "        out1 = out1[:, :, :min_len]\n",
    "        out2 = out2[:, :, :min_len]\n",
    "\n",
    "        # Concatenate the outputs from both branches\n",
    "        out = torch.cat([out1, out2], dim=1)  # (batch, 2*cnn_filters, seq_len_x)\n",
    "        out = self.bn(out)\n",
    "\n",
    "        # Reshape back for LSTM\n",
    "        out = out.permute(2, 0, 1)  # (seq_len, batch, features)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = out[-1,:,:]  # Taking the last output\n",
    "        out = self.fc(out)  # Fully connected layer\n",
    "        return out\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    t0 = time.time()  # Start time for training duration\n",
    "\n",
    "    # Read and preprocess the data\n",
    "    data = pd.read_csv('tsla_history.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.sort_values('Date', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Calculate additional features: Returns, Moving Averages, and RSI\n",
    "    data['Return'] = data['Close'].pct_change()\n",
    "    data['MA5'] = data['Close'].rolling(5).mean()\n",
    "    data['MA10'] = data['Close'].rolling(10).mean()\n",
    "    data['RSI'] = compute_RSI(data['Close'], window=14)\n",
    "\n",
    "    # Drop rows with NA values\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Select features and normalize the data\n",
    "    features = ['Close', 'Open', 'High', 'Low', 'Volume', 'MA5', 'MA10', 'RSI']\n",
    "    data_features = data[features].astype('float32').values\n",
    "    close_prices = data['Close'].astype('float32').values\n",
    "\n",
    "    max_features = np.max(data_features, axis=0)\n",
    "    min_features = np.min(data_features, axis=0)\n",
    "    data_features = (data_features - min_features) / (max_features - min_features)\n",
    "\n",
    "    max_close = np.max(close_prices)\n",
    "    min_close = np.min(close_prices)\n",
    "    close_prices = (close_prices - min_close) / (max_close - min_close)\n",
    "\n",
    "    # Create dataset for training, validation, and testing\n",
    "    dataset_x, dataset_y = create_dataset(data_features, close_prices, DAYS_FOR_TRAIN)\n",
    "    total_size = len(dataset_x)\n",
    "    train_size = int(total_size * 0.7)\n",
    "    val_size = int(total_size * 0.2)\n",
    "\n",
    "    train_x = dataset_x[:train_size]\n",
    "    train_y = dataset_y[:train_size]\n",
    "    val_x = dataset_x[train_size:train_size + val_size]\n",
    "    val_y = dataset_y[train_size:train_size + val_size]\n",
    "    test_x = dataset_x[train_size + val_size:]\n",
    "    test_y = dataset_y[train_size + val_size:]\n",
    "\n",
    "    # Extract dates for plotting\n",
    "    dates = data['Date'].values\n",
    "    train_dates = dates[DAYS_FOR_TRAIN:train_size + DAYS_FOR_TRAIN]\n",
    "    val_dates = dates[train_size + DAYS_FOR_TRAIN:train_size + val_size + DAYS_FOR_TRAIN]\n",
    "    test_dates = dates[train_size + val_size + DAYS_FOR_TRAIN:]\n",
    "\n",
    "    # Transpose and reshape the datasets for training\n",
    "    train_x = train_x.transpose(1, 0, 2)\n",
    "    train_y = train_y.reshape(-1, 1)\n",
    "    val_x = val_x.transpose(1, 0, 2)\n",
    "    val_y = val_y.reshape(-1)\n",
    "    test_x = test_x.transpose(1, 0, 2)\n",
    "    test_y = test_y.reshape(-1)\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    train_x = torch.from_numpy(train_x).float()\n",
    "    train_y = torch.from_numpy(train_y).float()\n",
    "    val_x = torch.from_numpy(val_x).float()\n",
    "    test_x = torch.from_numpy(test_x).float()\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = Enhanced_CNN_LSTM(input_size=len(features), hidden_size=32, output_size=1, cnn_filters=64)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(400):\n",
    "        model.train()\n",
    "        out = model(train_x)\n",
    "        loss = loss_function(out, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch: {i + 1}, Loss: {loss.item():.5f}')\n",
    "\n",
    "    # Evaluate the model on validation and test sets\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred = model(train_x).view(-1).data.numpy()\n",
    "        val_pred = model(val_x).view(-1).data.numpy()\n",
    "        test_pred = model(test_x).view(-1).data.numpy()\n",
    "\n",
    "    # Rescale predictions back to original scale\n",
    "    train_pred_real = train_pred * (max_close - min_close) + min_close\n",
    "    val_pred_real = val_pred * (max_close - min_close) + min_close\n",
    "    test_pred_real = test_pred * (max_close - min_close) + min_close\n",
    "\n",
    "    train_y_real = train_y.numpy().flatten() * (max_close - min_close) + min_close\n",
    "    val_y_real = val_y * (max_close - min_close) + min_close\n",
    "    test_y_real = test_y * (max_close - min_close) + min_close\n",
    "\n",
    "    # Compute directional movements for accuracy metrics\n",
    "    def get_direction(y):\n",
    "        return (np.diff(y) > 0).astype(int)\n",
    "\n",
    "    train_true_dir = get_direction(train_y_real)\n",
    "    train_pred_dir = get_direction(train_pred_real)\n",
    "\n",
    "    val_true_dir = get_direction(val_y_real)\n",
    "    val_pred_dir = get_direction(val_pred_real)\n",
    "\n",
    "    test_true_dir = get_direction(test_y_real)\n",
    "    test_pred_dir = get_direction(test_pred_real)\n",
    "\n",
    "    # Helper function to print evaluation metrics\n",
    "    def print_metrics(name, y_true, y_pred):\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        print(f\"{name} Set Metrics:\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "    # Print metrics for train, validation, and test sets\n",
    "    print_metrics(\"Train\", train_true_dir, train_pred_dir)\n",
    "    print_metrics(\"Validation\", val_true_dir, val_pred_dir)\n",
    "    print_metrics(\"Test\", test_true_dir, test_pred_dir)\n",
    "\n",
    "    # Plot the real and predicted prices\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_dates, train_y_real, label='Train Set', color='blue')\n",
    "    plt.plot(train_dates, train_pred_real, label='Prediction', color='green')\n",
    "\n",
    "    plt.plot(val_dates, val_y_real, label='Validation Set', color='orange')\n",
    "    plt.plot(val_dates, val_pred_real, color='green')\n",
    "\n",
    "    plt.plot(test_dates, test_y_real, label='Test Set', color='red')\n",
    "    plt.plot(test_dates, test_pred_real, color='green')\n",
    "\n",
    "    plt.title('Enhanced CNN+LSTM Regression Predicted Prices')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price($)')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the duration of the training process\n",
    "    t1 = time.time()\n",
    "    print(f'Training completed in {(t1 - t0) / 60:.2f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
